{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leukschrauber/Assignments/blob/main/assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "\n",
        "matplotlib.rc('image', cmap='Greys')"
      ],
      "metadata": {
        "id": "en76RyfuIKwm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.MNIST_SAMPLE)\n",
        "Path.BASE_PATH = path\n",
        "threes = (path/'train'/'3').ls().sorted()\n",
        "sevens = (path/'train'/'7').ls().sorted()\n",
        "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
        "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
        "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
        "stacked_threes = torch.stack(three_tensors).float()/255\n",
        "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
        "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n",
        "\n",
        "valid_3_tens = torch.stack([tensor(Image.open(o)) \n",
        "                            for o in (path/'valid'/'3').ls()])\n",
        "valid_3_tens = valid_3_tens.float()/255\n",
        "valid_7_tens = torch.stack([tensor(Image.open(o)) \n",
        "                            for o in (path/'valid'/'7').ls()])\n",
        "valid_7_tens = valid_7_tens.float()/255\n",
        "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\n",
        "valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)"
      ],
      "metadata": {
        "id": "SuJzDzfPIhzB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theory\n",
        "\n",
        "In the following assignment, your task is to complete the MNIST Basics chapter. It is best to repeat everything from last week and try to answer the following questions. Afterwards you have to summarize the learned facts with two programming tasks.\n",
        "\n",
        "**What is \"torch.cat()\" and \".view(-1, 28*28)\" doing in the beginning of the \"The MNIST Loss Function\" chapter?**\n",
        "\n",
        "torch.cat concatenates two tensors, meaning that it appends the one to the other.\n",
        "\n",
        "torch.view is able to manipulate the rank of tensors. In this, example where we had a tensor of shape (N, 28, 28), view reduces the dimensions by 1, resulting in a tensor of shape (N, 28*28).\n",
        "\n",
        "**Can you draw the neuronal network, which is manually trained in chapter \"The MNIST Loss Function\"?**\n",
        "\n",
        "In the chapter \"The MNIST Loss Function\", a neuronal network with a single input layer of 784 inputs and a single bias is trained. The Sigmoid function is applied as a step function at the end of the network to map the values to a number between -1 and 1."
      ],
      "metadata": {
        "id": "iIBgQ5f43H6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = nn.Linear(28*28,1)\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot\n",
        "\n",
        "make_dot(linear_model(valid_x).mean(), params=dict(linear_model.named_parameters()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "ZSHzFxFnIFW2",
        "outputId": "74db85de-b397-4edb-952e-09caf4ad48c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.0+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"216pt\" height=\"336pt\"\n viewBox=\"0.00 0.00 216.00 336.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 332)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-332 212,-332 212,4 -4,4\"/>\n<!-- 140465456983120 -->\n<g id=\"node1\" class=\"node\">\n<title>140465456983120</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"130.5,-31 76.5,-31 76.5,0 130.5,0 130.5,-31\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 140465593435088 -->\n<g id=\"node2\" class=\"node\">\n<title>140465593435088</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 56,-86 56,-67 151,-67 151,-86\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n</g>\n<!-- 140465593435088&#45;&gt;140465456983120 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140465593435088&#45;&gt;140465456983120</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-66.79C103.5,-60.07 103.5,-50.4 103.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-41.19 103.5,-31.19 100,-41.19 107,-41.19\"/>\n</g>\n<!-- 140465593443488 -->\n<g id=\"node3\" class=\"node\">\n<title>140465593443488</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"154,-141 53,-141 53,-122 154,-122 154,-141\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 140465593443488&#45;&gt;140465593435088 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140465593443488&#45;&gt;140465593435088</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-121.75C103.5,-114.8 103.5,-104.85 103.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-96.09 103.5,-86.09 100,-96.09 107,-96.09\"/>\n</g>\n<!-- 140465593446416 -->\n<g id=\"node4\" class=\"node\">\n<title>140465593446416</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 140465593446416&#45;&gt;140465593443488 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140465593446416&#45;&gt;140465593443488</title>\n<path fill=\"none\" stroke=\"black\" d=\"M59.25,-176.75C66.97,-169.03 78.4,-157.6 87.72,-148.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"90.31,-150.64 94.91,-141.09 85.36,-145.69 90.31,-150.64\"/>\n</g>\n<!-- 140465456982960 -->\n<g id=\"node5\" class=\"node\">\n<title>140465456982960</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-262 23.5,-262 23.5,-232 77.5,-232 77.5,-262\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 140465456982960&#45;&gt;140465593446416 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140465456982960&#45;&gt;140465593446416</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-231.84C50.5,-224.21 50.5,-214.7 50.5,-206.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-206.27 50.5,-196.27 47,-206.27 54,-206.27\"/>\n</g>\n<!-- 140465593448048 -->\n<g id=\"node6\" class=\"node\">\n<title>140465593448048</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"196,-196 119,-196 119,-177 196,-177 196,-196\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 140465593448048&#45;&gt;140465593443488 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140465593448048&#45;&gt;140465593443488</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.58,-176.75C140.72,-169.03 129.07,-157.6 119.58,-148.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"121.84,-145.6 112.25,-141.09 116.94,-150.59 121.84,-145.6\"/>\n</g>\n<!-- 140465593443392 -->\n<g id=\"node7\" class=\"node\">\n<title>140465593443392</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-256.5 107,-256.5 107,-237.5 208,-237.5 208,-256.5\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 140465593443392&#45;&gt;140465593448048 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140465593443392&#45;&gt;140465593448048</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.5,-237.37C157.5,-229.25 157.5,-216.81 157.5,-206.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"161,-206.17 157.5,-196.17 154,-206.17 161,-206.17\"/>\n</g>\n<!-- 140465493287824 -->\n<g id=\"node8\" class=\"node\">\n<title>140465493287824</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"193,-328 122,-328 122,-298 193,-298 193,-328\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">weight</text>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (1, 784)</text>\n</g>\n<!-- 140465493287824&#45;&gt;140465593443392 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140465493287824&#45;&gt;140465593443392</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.5,-297.8C157.5,-288.7 157.5,-276.79 157.5,-266.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"161,-266.84 157.5,-256.84 154,-266.84 161,-266.84\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7fc0b1ccff40>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Why is it not possible to use the accuracy as loss function?**\n",
        "\n",
        "The accuracy for a single prediction is defined as 0 (false) for wrong prediction and 1 (true) for a true prediction. The outcome of a network will always be mapped to one of these two constant values. The slope of a constant is always zero. It is infinity for the border between the two value mappings. As we need slopes for backpropagation in Stochastic Gradient Descent, a function with a slop of zero is not suitable.\n",
        "\n",
        "\n",
        "**What is the defined `mnist_loss` function doing? **\n",
        "\n",
        "\n",
        "```\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "```\n",
        "\n",
        "This function will measure how distant each prediction is from 1 if it should be 1, and how distant it is from 0 if it should be 0, and then it will take the mean of all those distances. It works similary to a ternary if-else operator, just on a pyTorch Tensor.\n",
        "\n",
        "**Why do we need additionaly the sigmoid() function? What is it technically in our TLU?**\n",
        "\n",
        "The sigmoid function maps the output of the network to a value between -1 and 1. Thus, every value < 0 can be defined as one class and every value > 0 can be defined as the other class in our classification problem. Additionally, it has a well-defined slope for every x enabling a meaningful Stochastic Gradient Descent. It is our step function.\n",
        "\n",
        "**Again, what are mini batches, why are we using them and why should they be shuffeld?**\n",
        "\n",
        "The question is how many data items are suitable for performing a single optimization step. The size should be just big enough to provide a consistent and meaningful stochastic gradient descent. A single item will be rather inconsistent and the whole data set (epoch) will take too long often. \n",
        "\n",
        "Shuffling the batches helps reduce the risk of overfitting and will overall merge into better results as local optima are mitigated."
      ],
      "metadata": {
        "id": "QBpyA8p4H4Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Part\n",
        "\n",
        "Try to understand all parts of the code needed to manually train a single TLU/Perceptron, so use and copy all parts of the code from \"First Try: Pixel Similarity\" to the \"Putting it all together\" chapter. In the second step, use an optimizer, a second layer, and a ReLU as a hidden activation function to train a simple neural network. When copying the code, think carefully about what you really need and how you can summarize it as compactly as possible. (Probably each attempt requires about 15 lines of code.)"
      ],
      "metadata": {
        "id": "aoQq7z5D3XXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TxwyNuzj3DYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d1c986-1dd1-4d9c-8115-3a504dfdd678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4947 0.5935 0.7938 0.8886 0.9281 0.9384 0.9477 0.954 0.9589 0.9613 0.9643 0.9643 0.9677 0.9667 0.9676 0.9681 0.9691 0.9706 0.9706 0.9711 "
          ]
        }
      ],
      "source": [
        "# Used to initialize weights and bias\n",
        "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
        "\n",
        "# Matrix multiplication of a input matrix and weights + bias. This is the network.\n",
        "def linear1(xb): return xb@weights + bias\n",
        "\n",
        "# Loss function: Difference between sigmoid and correct value.\n",
        "def mnist_loss(predictions, targets):\n",
        "    predictions = predictions.sigmoid()\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "\n",
        "# Calculates the gradient for the Stochastic Gradient Descent\n",
        "def calc_grad(xb, yb, model):\n",
        "    preds = model(xb)\n",
        "    loss = mnist_loss(preds, yb)\n",
        "    loss.backward()\n",
        "\n",
        "# Trains an epoch with n mini batches in the dataloader.\n",
        "# Performs an optimization step for every mini batch using the learning rate.\n",
        "def train_epoch(model, lr, params):\n",
        "    for xb,yb in dl:\n",
        "        calc_grad(xb, yb, model)\n",
        "        for p in params:\n",
        "            p.data -= p.grad*lr\n",
        "            p.grad.zero_()\n",
        "\n",
        "# Calculates the accuracy for the predictions. Not really necessary for the network\n",
        "# but great for human understanding.\n",
        "def batch_accuracy(xb, yb):\n",
        "    preds = xb.sigmoid()\n",
        "    correct = (preds>0.5) == yb\n",
        "    return correct.float().mean()\n",
        "\n",
        "# Using the validation dataloader, calculates an accuracy for preds.\n",
        "def validate_epoch(model):\n",
        "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
        "    return round(torch.stack(accs).mean().item(), 4)\n",
        "\n",
        "# Weights and bias\n",
        "weights = init_params((28*28,1))\n",
        "bias = init_params(1)\n",
        "# The data sets and data loaders for both training and validation.\n",
        "dl = DataLoader(list(zip(train_x,train_y)), batch_size=256)\n",
        "valid_dl = DataLoader(list(zip(valid_x,valid_y)), batch_size=256)\n",
        "\n",
        "# The training loop. 20 epochs. 1.0 Learning Rate\n",
        "for i in range(20):\n",
        "    train_epoch(linear1, 1.0, (weights,bias))\n",
        "    print(validate_epoch(linear1), end=' ')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model: 784 weights -> 30 outputs. ReLU Activation Function. 30 Inputs -> 1 Output\n",
        "simple_net = nn.Sequential(\n",
        "    nn.Linear(28*28,30),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(30,1)\n",
        ")\n",
        "\n",
        "# Loss function: Difference between sigmoid and correct value.\n",
        "def mnist_loss(predictions, targets):\n",
        "    predictions = predictions.sigmoid()\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "\n",
        "# Calculates the accuracy for the predictions. Not really necessary for the network\n",
        "# but great for human understanding.\n",
        "def batch_accuracy(xb, yb):\n",
        "    preds = xb.sigmoid()\n",
        "    correct = (preds>0.5) == yb\n",
        "    return correct.float().mean()\n",
        "\n",
        "# The data sets and data loaders for both training and validation.\n",
        "dl = DataLoader(list(zip(train_x,train_y)), batch_size=256)\n",
        "valid_dl = DataLoader(list(zip(valid_x,valid_y)), batch_size=256)\n",
        "\n",
        "# Learner takes test and validation dataset, the model, the SGD for optimization, \n",
        "# the minst_loss function for calculating the loss, the batch_accuracy for validation\n",
        "learn = Learner(DataLoaders(dl, valid_dl), simple_net, opt_func=SGD,\n",
        "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
        "learn.fit(40, 0.1)\n",
        "\n",
        "plt.plot(L(learn.recorder.values).itemgot(2));"
      ],
      "metadata": {
        "id": "UGsLRFtMbyRZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09699b91-2199-43ed-d5b3-9338f27cbb44"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>batch_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.310629</td>\n",
              "      <td>0.406701</td>\n",
              "      <td>0.508341</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.143736</td>\n",
              "      <td>0.228092</td>\n",
              "      <td>0.803238</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.079770</td>\n",
              "      <td>0.114890</td>\n",
              "      <td>0.916585</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.052617</td>\n",
              "      <td>0.077463</td>\n",
              "      <td>0.941119</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.040036</td>\n",
              "      <td>0.060461</td>\n",
              "      <td>0.956330</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.033613</td>\n",
              "      <td>0.050974</td>\n",
              "      <td>0.964181</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.029924</td>\n",
              "      <td>0.045003</td>\n",
              "      <td>0.965653</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.027524</td>\n",
              "      <td>0.040925</td>\n",
              "      <td>0.967615</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.025788</td>\n",
              "      <td>0.037957</td>\n",
              "      <td>0.969087</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.024438</td>\n",
              "      <td>0.035687</td>\n",
              "      <td>0.971541</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.023338</td>\n",
              "      <td>0.033881</td>\n",
              "      <td>0.971541</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.022417</td>\n",
              "      <td>0.032398</td>\n",
              "      <td>0.973503</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.021629</td>\n",
              "      <td>0.031150</td>\n",
              "      <td>0.975466</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.020948</td>\n",
              "      <td>0.030074</td>\n",
              "      <td>0.975957</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.020349</td>\n",
              "      <td>0.029135</td>\n",
              "      <td>0.976448</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.019819</td>\n",
              "      <td>0.028302</td>\n",
              "      <td>0.976448</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.019345</td>\n",
              "      <td>0.027560</td>\n",
              "      <td>0.977920</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.018917</td>\n",
              "      <td>0.026892</td>\n",
              "      <td>0.978410</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.018529</td>\n",
              "      <td>0.026287</td>\n",
              "      <td>0.978901</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.018174</td>\n",
              "      <td>0.025738</td>\n",
              "      <td>0.978901</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.017846</td>\n",
              "      <td>0.025237</td>\n",
              "      <td>0.979882</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>0.024778</td>\n",
              "      <td>0.979882</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.017262</td>\n",
              "      <td>0.024358</td>\n",
              "      <td>0.979882</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.017000</td>\n",
              "      <td>0.023970</td>\n",
              "      <td>0.980864</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.016754</td>\n",
              "      <td>0.023613</td>\n",
              "      <td>0.980864</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.016522</td>\n",
              "      <td>0.023284</td>\n",
              "      <td>0.980864</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.016304</td>\n",
              "      <td>0.022978</td>\n",
              "      <td>0.981354</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.016098</td>\n",
              "      <td>0.022695</td>\n",
              "      <td>0.981354</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.015903</td>\n",
              "      <td>0.022432</td>\n",
              "      <td>0.981354</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.015718</td>\n",
              "      <td>0.022187</td>\n",
              "      <td>0.981354</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.015541</td>\n",
              "      <td>0.021958</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.015373</td>\n",
              "      <td>0.021745</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.015213</td>\n",
              "      <td>0.021545</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.015059</td>\n",
              "      <td>0.021358</td>\n",
              "      <td>0.982826</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.014912</td>\n",
              "      <td>0.021182</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.014771</td>\n",
              "      <td>0.021017</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.014636</td>\n",
              "      <td>0.020861</td>\n",
              "      <td>0.981845</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.014505</td>\n",
              "      <td>0.020713</td>\n",
              "      <td>0.981845</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.014379</td>\n",
              "      <td>0.020574</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.014258</td>\n",
              "      <td>0.020441</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}